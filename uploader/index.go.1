package uploader

import (
	"bytes"
	"database/sql"
	"fmt"
	"io"
	"strings"
	"time"

	"github.com/lomik/carbon-clickhouse/helper/RowBinary"
	_ "github.com/mailru/go-clickhouse"
)

type Index struct {
	*cached
}

var _ Uploader = &Index{}
var _ UploaderWithReset = &Index{}

const ReverseLevelOffset = 10000
const TreeLevelOffset = 20000
const ReverseTreeLevelOffset = 30000

const DefaultTreeDate = 42 // 1970-02-12

const indexQuery = "SELECT Date, Path FROM graphite_index WHERE %s GROUP BY Path, Date ORDER By Path, Date ASC"

type indexRow struct {
	days      uint16
	name      []byte
	sname     string
	found     bool
	foundTree bool
}

func NewIndex(base *Base) *Index {
	u := &Index{}
	u.cached = newCached(base)
	u.cached.parser = u.parseFile
	return u
}

func filterAddDatePath(filterMap map[uint16]strings.Builder, ir indexRow, days uint16) {
	sb, ok := filterMap[days]
	if ok {
		sb.WriteString(", '" + ir.sname + "'")
	} else {
		sb.WriteString("'" + ir.sname + "'")
		filterMap[days] = sb
	}
}

func filterAddPath(sb *strings.Builder, ir indexRow, treeDate uint16) {
	filterAddDatePath(filterMap, ir, ir.days)
	filterAddDatePath(filterMap, ir, treeDate)
}

func filterString(filterMap map[uint16]strings.Builder) string {
	var sb strings.Builder
	first := true
	for date, v := range filterMap {
		t := time.Unix(86400*int64(date), 0)
		if first {
			first = false
			sb.WriteString(" (Date =")
		} else {
			sb.WriteString(" OR (Date =")
		}
		s := fmt.Sprintf("'%04d-%02d-%02d'", t.Year(), t.Month(), t.Day())
		sb.WriteString(s)
		sb.WriteString(" AND Path IN (")
		sb.WriteString(v.String())
		sb.WriteString("))")
	}

	return sb.String()
}

func (u *Index) cacheQueryCheck(connect *sql.DB, filterMap map[uint16]strings.Builder, indexes map[string]indexRow, treeDate uint16) error {
	query := fmt.Sprintf(indexQuery, filterString(filterMap))
	rows, err := connect.Query(query)
	if err != nil {
		return err
	}
	var treePath string
	for rows.Next() {
		var date time.Time
		var path string
		if err := rows.Scan(&date, &path); err != nil {
			return err
		}
		d := RowBinary.SlowTimeToDays(date)
		key := fmt.Sprintf("%d:%s", d, path)
		if d == treeDate {
			treePath = path
		} else {
			v, ok := indexes[key]
			if ok {
				v.found = true
				if treePath == path && !v.foundTree {
					v.foundTree = true
				}
				indexes[key] = v
			} else {
				return fmt.Errorf("key '%s' not found during index lookup, may be wrong filter generated", key)
			}
		}
	}
	return nil
}

func (u *Index) cacheBatchRecheck(indexes map[string]indexRow, treeDate uint16) (map[string]bool, error) {
	newSeries := make(map[string]bool)
	if len(indexes) == 0 {
		return newSeries, nil
	}

	connect, err := sql.Open("clickhouse", u.config.URL)
	if err != nil {
		return nil, err
	}
	if err := connect.Ping(); err != nil {
		return nil, err
	}

	n := 0
	filterMap := make(map[uint16]strings.Builder)
	for _, v := range indexes {
		if n > 5000 {
			err = u.cacheQueryCheck(connect, filterMap, indexes, treeDate)
			if err != nil {
				return nil, err
			}

			filterMap = make(map[uint16]strings.Builder)
			n = 0
		}
		filterAddPath(filterMap, v, treeDate)
		n++
	}

	if n > 0 {
		err = u.cacheQueryCheck(connect, filterMap, indexes, treeDate)
		if err != nil {
			return nil, err
		}

		filterMap = nil
	}

	for key, v := range indexes {
		if !v.found {
			newSeries[key] = true
		}
	}
	return newSeries, nil
}

func (u *Index) parseFile(filename string, out io.Writer) (uint64, map[string]bool, error) {
	var reader *RowBinary.Reader
	var err error
	var n uint64

	reader, err = RowBinary.NewReader(filename, false)
	if err != nil {
		return n, nil, err
	}
	defer reader.Close()

	version := uint32(time.Now().Unix())
	nocacheSeries := make(map[string]indexRow)
	newUniq := make(map[string]bool)
	wb := RowBinary.GetWriteBuffer()

	var level, index, l int
	var p []byte

	treeDate := uint16(DefaultTreeDate)
	if !u.config.TreeDate.IsZero() {
		treeDate = RowBinary.TimestampToDays(uint32(u.config.TreeDate.Unix()))
	}

LineLoop:
	for {
		name, err := reader.ReadRecord()
		if err != nil { // io.EOF or corrupted file
			break
		}

		// skip tagged
		if bytes.IndexByte(name, '?') >= 0 {
			continue
		}

		sname := unsafeString(name)
		key := fmt.Sprintf("%d:%s", reader.Days(), sname)

		if u.existsCache.Exists(key) {
			continue LineLoop
		}

		if _, ok := nocacheSeries[key]; ok {
			continue LineLoop
		}

		nocacheSeries[key] = indexRow{
			days:  reader.Days(),
			name:  name,
			sname: sname,
		}
	}

	newSeries, err := u.cacheBatchRecheck(nocacheSeries, treeDate)
	if err != nil {
		return n, nil, err
	}

	for _, v := range nocacheSeries {
		if v.found {
			continue
		}
		n++

		wb.Reset()

		level = pathLevel(v.name)
		// Direct path with date
		wb.WriteUint16(v.days)
		wb.WriteUint32(uint32(level))
		wb.WriteBytes(v.name)
		wb.WriteUint32(version)

		reverseName := RowBinary.ReverseBytes(v.name)

		// Reverse path with date
		wb.WriteUint16(v.days)
		wb.WriteUint32(uint32(level + ReverseLevelOffset))
		wb.WriteBytes(reverseName)
		wb.WriteUint32(version)

		// Tree
		wb.WriteUint16(treeDate)
		wb.WriteUint32(uint32(level + TreeLevelOffset))
		wb.WriteBytes(v.name)
		wb.WriteUint32(version)

		p = v.name
		l = level
		for l--; l > 0; l-- {
			index = bytes.LastIndexByte(p, '.')
			if newUniq[unsafeString(p[:index+1])] {
				break
			}

			newUniq[string(p[:index+1])] = true

			wb.WriteUint16(treeDate)
			wb.WriteUint32(uint32(l + TreeLevelOffset))
			wb.WriteBytes(p[:index+1])
			wb.WriteUint32(version)

			p = p[:index]
		}

		if !v.foundTree {
			// Reverse path without date
			wb.WriteUint16(treeDate)
			wb.WriteUint32(uint32(level + ReverseTreeLevelOffset))
			wb.WriteBytes(reverseName)
			wb.WriteUint32(version)
		}

		_, err = out.Write(wb.Bytes())
		if err != nil {
			return n, nil, err
		}
	}

	wb.Release()

	return n, newSeries, nil
}
